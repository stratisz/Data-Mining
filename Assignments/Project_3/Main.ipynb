{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Απαλλακτική Εργασία ΤΕΔε part 2 (do stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statistics for standard deviation \n",
    "from statistics import stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = r'./clean_test.csv'\n",
    "train_name = r'./clean_train.csv'\n",
    "\n",
    "Train_df1 = pd.read_csv(train_name, sep=',', usecols=['Insult','Comment'])\n",
    "Test_df1 = pd.read_csv(test_name, sep=',', usecols=['id','Insult','Comment']) # with labels\n",
    "\n",
    "Train_df = Train_df1['Comment'] #without the labels\n",
    "Test_df = Test_df1[['id','Comment']] #without the labels\n",
    "\n",
    "labels_train = Train_df1['Insult'].to_numpy()  #training labels\n",
    "labels_test = Test_df1['Insult'].to_numpy() #test labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error popped up during count vectorizer: np.nan is an invalid document, expected byte or unicode string.\n",
    "\n",
    "So we change them into 'str' as recommended here: https://stackoverflow.com/questions/39303912/tfidfvectorizer-in-scikit-learn-valueerror-np-nan-is-an-invalid-document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['like this if you are a tribe fan',\n",
       "       'you re idiot                       ',\n",
       "       'i am a woman babs  and the only war on women i see is coming from jackazzes like you   i don t need your protection or your ignorant rhetoric masquerading as representing my best interests   ',\n",
       "       ...,\n",
       "       'sweetie pie is looking very much like her cousin maggie e   what a beauty this little one is  love you all to pieces   ',\n",
       "       'ball4real where are you with your miami g ayness',\n",
       "       'man    if you are a 3 point shooter  you must love playing with wade and bron  '],\n",
       "      dtype='<U1387')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_df.values.astype('U')\n",
    "Test_df.Comment.values.astype('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our scoring labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_labels=['accuracy','f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score maybe not needed ==> balanced_accuracy and f1_score not needed\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compute_scores(prediction, labels_test, model, train, labels_train):\n",
    "    #scores is a list of scores\n",
    "    \n",
    "    accuracy = accuracy_score(labels_test, prediction)\n",
    "    \n",
    "    f1_scores = cross_val_score(model, train, labels_train, cv=10, scoring='f1')\n",
    "    f1_avg = np.mean(f1_scores)\n",
    "    f1_stdev = stdev(f1_scores)\n",
    "    \n",
    "    \n",
    "    #f1_scores = cross_val_score(mnb, counts_train, labels_train, cv=10, scoring='f1_macro')\n",
    "\n",
    "    \n",
    "    print(\"f1 is (mean : %f) and (standard deviation: %f)\" % ( f1_avg, f1_stdev ) )\n",
    "    print(\"accuracy is %f\" % ( accuracy ) )\n",
    "          \n",
    "    #return [accuracy, [f1_avg,f1_stdev]]\n",
    "    return [accuracy, f1_avg , f1_stdev ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer()\n",
    "\n",
    "Για κάθε πείραμα NaiveBayes, θα φτιάχνουμε καινούργιο CountVectorizer (με τις οποιες βελτιώσεις)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english')\n",
    "\n",
    "#counts_train = bow_vectorizer.fit_transform(Train_df.values.astype('U'))\n",
    "#counts_test = bow_vectorizer.transform(Test_df.Comment.values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "<b>ΣΗΜΑΝΤΙΚΟ</b>: Για τα ερωτήματα Naive Bayes θα χρησιμοποιήσουμε το μοντέλο <b>BernoulliNB</b> είναι σαν το MultinomialNB αλλά για Binary Classification (έχει και την  παράμετρο <b>alpha</b> για το Laplace smoothing)\n",
    "\n",
    "Σημαντικό2: Δεν θα κάνω GridSearchCV για τα hyperparameters (για νβγάλω πιο ξεκάθαρα αποτελέσματα). Θα χρησιμοποιήσω:\n",
    "\n",
    "1) <b> alpha = 1.0e-10 </b> (almost no smoothing, γιατί (=1) είναι Laplace smoothing και (<1) είναι Lidstone smoothing). Αν βαλώ 0.0 , λέει ότι είναι κάτω από minimum, οπότε βάζει αυτόματα την παραπάνω τιμή\n",
    "\n",
    "2) binarize = 0.0 (default)\n",
    "\n",
    "3) class_prior = None (default)\n",
    "\n",
    "4) fit_prior = True (βοηθάει τα scores, αργεί λίγο στο training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes1\n",
    "Χωρίς βελτιώσεις"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=1000)\n",
    "\n",
    "counts_train = bow_vectorizer.fit_transform(Train_df.values.astype('U'))\n",
    "counts_test = bow_vectorizer.transform(Test_df.Comment.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bNB1 = BernoulliNB(alpha=1.0e-10,fit_prior=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1e-10, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bNB1.fit(counts_train,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bNB1_pred = bNB1.predict(counts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 is (mean : 0.562336) and (standard deviation: 0.027070)\n",
      "accuracy is 0.523490\n"
     ]
    }
   ],
   "source": [
    "bNB1_scores = Compute_scores(bNB1_pred, labels_test, bNB1, counts_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes2\n",
    "Εδώ κάνουμε <b>lemmatization</b> στον Count vectorizer όπως περιγράφεται στο <b>6.2.3.10</b>: https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "         self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for our CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=1000, tokenizer=LemmaTokenizer()) #HERE\n",
    "\n",
    "counts_train = bow_vectorizer.fit_transform(Train_df.values.astype('U'))\n",
    "counts_test = bow_vectorizer.transform(Test_df.Comment.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bNB2 = BernoulliNB(alpha=1.0e-10,fit_prior=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1e-10, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bNB2.fit(counts_train,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bNB2_pred = bNB2.predict(counts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 is (mean : 0.566353) and (standard deviation: 0.027171)\n",
      "accuracy is 0.522148\n"
     ]
    }
   ],
   "source": [
    "bNB2_scores = Compute_scores(bNB2_pred, labels_test, bNB2, counts_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes3\n",
    "Αφαιρούμε τα <b>stopwords</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') #HERE\n",
    "\n",
    "counts_train = bow_vectorizer.fit_transform(Train_df.values.astype('U'))\n",
    "counts_test = bow_vectorizer.transform(Test_df.Comment.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bNB3 = BernoulliNB(alpha=1.0e-10,fit_prior=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1e-10, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bNB3.fit(counts_train,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bNB3_pred = bNB3.predict(counts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 is (mean : 0.592411) and (standard deviation: 0.029309)\n",
      "accuracy is 0.666667\n"
     ]
    }
   ],
   "source": [
    "bNB3_scores = Compute_scores(bNB3_pred, labels_test, bNB3, counts_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes4\n",
    "Χρησιμοποιούμε <b>bigrams</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=1000, ngram_range=(2,2)) #HERE\n",
    "\n",
    "counts_train = bow_vectorizer.fit_transform(Train_df.values.astype('U'))\n",
    "counts_test = bow_vectorizer.transform(Test_df.Comment.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bNB4 = BernoulliNB(alpha=1.0e-10,fit_prior=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Fit out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1e-10, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bNB4.fit(counts_train,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bNB4_pred = bNB4.predict(counts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 is (mean : 0.571800) and (standard deviation: 0.043224)\n",
      "accuracy is 0.608949\n"
     ]
    }
   ],
   "source": [
    "bNB4_scores = Compute_scores(bNB4_pred, labels_test, bNB4, counts_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes5\n",
    "Χρησιμοποιύμε <b> Laplace Smoothing </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=1000)\n",
    "\n",
    "counts_train = bow_vectorizer.fit_transform(Train_df.values.astype('U'))\n",
    "counts_test = bow_vectorizer.transform(Test_df.Comment.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bNB5 = BernoulliNB(alpha=1.0,fit_prior=True) #HERE alpha=1.0 => Laplace smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Fit out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bNB5.fit(counts_train,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bNB5_pred = bNB5.predict(counts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 is (mean : 0.594627) and (standard deviation: 0.030683)\n",
      "accuracy is 0.534228\n"
     ]
    }
   ],
   "source": [
    "bNB5_scores = Compute_scores(bNB5_pred, labels_test, bNB5, counts_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Αποτελέσματα \n",
    "Θα τα βάλω σε ένα dataframe για να φαίνονται καλύτερα"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 (avg)</th>\n",
       "      <th>F1 (τυπική απόκλιση)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NB baseline</th>\n",
       "      <td>0.523490</td>\n",
       "      <td>0.562336</td>\n",
       "      <td>0.027070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB lemmatization</th>\n",
       "      <td>0.522148</td>\n",
       "      <td>0.566353</td>\n",
       "      <td>0.027171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB stopwords</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.592411</td>\n",
       "      <td>0.029309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB bigrams</th>\n",
       "      <td>0.608949</td>\n",
       "      <td>0.571800</td>\n",
       "      <td>0.043224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB Laplace Smoothing</th>\n",
       "      <td>0.534228</td>\n",
       "      <td>0.594627</td>\n",
       "      <td>0.030683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Accuracy  F1 (avg)  F1 (τυπική απόκλιση)\n",
       "NB baseline           0.523490  0.562336              0.027070\n",
       "NB lemmatization      0.522148  0.566353              0.027171\n",
       "NB stopwords          0.666667  0.592411              0.029309\n",
       "NB bigrams            0.608949  0.571800              0.043224\n",
       "NB Laplace Smoothing  0.534228  0.594627              0.030683"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [bNB1_scores,bNB2_scores,bNB3_scores,bNB4_scores,bNB5_scores] # a list of lists\n",
    "index1 = ['NB baseline', 'NB lemmatization', 'NB stopwords', 'NB bigrams', 'NB Laplace Smoothing']\n",
    "columns1 = ['Accuracy','F1 (avg)', 'F1 (τυπική απόκλιση)']\n",
    "\n",
    "NB_df = pd.DataFrame( data=list1, index=index1, columns=columns1)\n",
    "NB_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρήσεις:\n",
    "\n",
    "1) Τα καλύτερα αποτελέσματα έχουμε <b> αφαιρώντας τα stopwords</b> τόσο σε accuracy όσο και σε f1 (με μια σχετικά καλή διακύμανση τιμών f1, συγκριτικά με το minimum ==> baseline). Δείχνει πως το να αφαιρείς τα stopwords είναι αρκετά σημαντικό.\n",
    "\n",
    "2) Ακολουθεί το <b> NB bigrams </b> ,αλλά εμφανίζει πολύ μεγαλύτερη διακύμανση  (0,43224 >> 0.29309)\n",
    "\n",
    "3) Το lemmatization πιθανώς να λειτουργούσε καλύτερα, αν δεν αφαιρούσαμε τα σημεία στίξης/punctuation (που απαιτεί η εκφώνηση).\n",
    "\n",
    "4) Τα <b> bigrams </b> είναι λογικό να δίνει καλύτερα αποτελέσματα, γιατί δίνει παραπάνω \"context\" στην πρόταση και γίνεται πιο εύκολα το classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2!\n",
    "Θα ασχοληθούμε με <b> POS tagging </b> , TF-IDF και τα μοντέλα SVM, random decision forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TfidfVectorizer()</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000,stop_words='english')\n",
    "\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(Train_df.values.astype('U'))\n",
    "tfidf_test = tfidf_vectorizer.transform(Test_df.Comment.values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging\n",
    "Επιδή θα χρειαστούμε να υπολογίσουμε κάποια fractions, θα χρησιμοποιήσουμε <b>collections.Counter </b>\n",
    "\n",
    "https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b\n",
    "\n",
    "Έχω προσθέσει και 2 <b> δικά μου fractions/features </b>\n",
    "\n",
    "1) <b>Personal Pronouns</b>\n",
    "\n",
    "2)<b> wh-adverb and wh-pronouns</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μας ενδιαφέρουν:\n",
    "\n",
    "1) adverb = (RB,RBR,RBS)\n",
    "\n",
    "2) verb = (VB,VBD,VBG,VBN.VBP,VBZ)\n",
    "\n",
    "3) adjective = (JJ,JJR,JJS)\n",
    "\n",
    "4) nouns = (NN,NNS,NNP,NNPS)\n",
    "\n",
    "### Προσθέτω και 2 δικά μου\n",
    "\n",
    "5) Personal Pronouns (PRP, PRP$): \"Who do you think you are?\" \"F*ck you\", \"You idiot\"\n",
    "\n",
    "6) wh-pronoun and wh-adverbs (WP,WP$,WRB): \"Whose fault do you think this is?\"\n",
    "\"What are you doing, idiot.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_tagging(Series,tfidf_array):\n",
    "    \n",
    "    #list1 = [] #list of lists\n",
    "    \n",
    "    adverb_list = [] #number for each sample\n",
    "    verb_list = []\n",
    "    adjective_list = []    \n",
    "    noun_list = []\n",
    "    \n",
    "    #########\n",
    "    personalp_list = []\n",
    "    wh_list =[]\n",
    "    \n",
    "    for comment in Series:\n",
    "    \n",
    "        tokens = nltk.word_tokenize(str(comment))\n",
    "        tag_list = nltk.pos_tag(tokens) #HERE, POS tagging\n",
    "        word_num = len(tag_list) # because this list is [(word1,tag),(word2,tag),.....]\n",
    "          \n",
    "        if(word_num == 0): #empty sentence\n",
    "            adverb_list.append(0)\n",
    "            verb_list.append(0)\n",
    "            adjective_list.append(0)\n",
    "            noun_list.append(0)\n",
    "            \n",
    "            personalp_list.append(0)\n",
    "            wh_list.append(0)\n",
    "            continue\n",
    "        \n",
    "    \n",
    "        counter = Counter()\n",
    "        for x in tag_list: # count the frequency for each tag\n",
    "            counter[x[1]] +=1\n",
    "    \n",
    "        \n",
    "        #1) adverb (RB,RBR,RBS)\n",
    "        sum_adverb = counter['RB'] + counter['RBR'] + counter['RBS'] + counter['RBS']\n",
    "        fraction_adverb = sum_adverb / word_num\n",
    "        adverb_list.append(fraction_adverb)\n",
    "        \n",
    "        # 2) verb (VB,VBD,VBG,VBN.VBP,VBZ)\n",
    "        sum_verb = counter['VB'] + counter['VBD'] + counter['VBG'] + counter['VBN'] + counter['VBP'] + counter['VBZ']\n",
    "        sum_verb += counter['MD'] #modal verbs\n",
    "        fraction_verb = sum_verb / word_num\n",
    "        verb_list.append(fraction_verb)\n",
    "        \n",
    "        # 3) adjective = (JJ,JJR,JJS)\n",
    "        sum_adjective = counter['JJ'] + counter['JJR'] + counter['JJS']\n",
    "        fraction_adjective = sum_adjective / word_num\n",
    "        adjective_list.append(fraction_adjective)\n",
    "        \n",
    "        #4) Nouns (NN,NNS,NNP,NNPS)\n",
    "        sum_noun = counter['NN'] + counter['NNS'] + counter['NNP'] + counter['NNPS']\n",
    "        fraction_noun = sum_noun / word_num\n",
    "        noun_list.append(fraction_noun)\n",
    "        \n",
    "        ###### The last 2 are mine\n",
    "        \n",
    "        # 5) Personal Pronoun\n",
    "        sum_personalp = counter['PRP'] + counter['PRP$']\n",
    "        fraction_personalp = sum_personalp / word_num\n",
    "        personalp_list.append(fraction_personalp)\n",
    "\n",
    "        # 6) wh-adverb and wh-pronoun\n",
    "        sum_wh = counter['WP'] + counter['WP$'] + counter['WRB']\n",
    "        fraction_wh = sum_wh / word_num\n",
    "        wh_list.append(fraction_wh)\n",
    "        \n",
    "        \n",
    "    #end of loop => perform concats\n",
    "    #print(tfidf_array.shape,len(adverb_list),len(verb_list),len(adjective_list),len(noun_list))\n",
    "    \n",
    "    dense_array = tfidf_array.todense()\n",
    "    \n",
    "    adverb_array = np.array(adverb_list)\n",
    "    verb_array = np.array(verb_list)\n",
    "    adjective_array = np.array(adjective_list)\n",
    "    noun_array = np.array(noun_list)\n",
    "    \n",
    "    personalp_array = np.array(personalp_list)\n",
    "    wh_array = np.array(wh_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #add these arrays as new columns to tfidf array\n",
    "    new_array = np.column_stack((dense_array,adverb_array,verb_array,adjective_array,noun_array))\n",
    "    new_array = np.column_stack((new_array,personalp_array,wh_array))\n",
    "    \n",
    "    #turn it back to sparse (tfidf was sparse)\n",
    "    new_array = sparse.csr_matrix(new_array)\n",
    "    \n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the compostite <b>train array </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = POS_tagging(Train_df,tfidf_train)\n",
    "pos_test = POS_tagging(Test_df.Comment,tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Συγκεκριμένα:\n",
    "\n",
    "## SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute <b> HyperParameters </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C = [1.0,5.0,10.0]\n",
    "#C = [1.0]\n",
    "\n",
    "#C=[1.0,10.0,100.0,1000.0,5000.0] # 1.0\n",
    "#C = [0.1,0.01,0.001,0.0001,0.00001] #0.1\n",
    "#C = [0.05,0.5] #0.5\n",
    "#C = [0.7 ,0.2] #0.7\n",
    "#C = [0.8,0.9] #0.8\n",
    "#C = [0.7,0.8] #0.8\n",
    "#C = [0.85,0.89] #0.85\n",
    "#C = [0.85,0.84,0.83,0.82,0.81] #0.82\n",
    "#C = [0.815,0.816,0.817,0.818,0.819,0.82, 0.821,0.822,0.823,0.824,0.825,0.826,0.827,0.828,0.829] # 0.82\n",
    "C = [0.82]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#kernel = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'] #not precomputed ==> need square matrix\n",
    "#kernel = ['linear', 'poly', 'rbf', 'sigmoid'] \n",
    "kernel = ['rbf'] #or sigmoid or linear\n",
    "\n",
    "\n",
    "\n",
    "#gamma = ['scale','auto'] #scale\n",
    "gamma = ['scale']\n",
    "\n",
    "cache_size = [1000] # MB\n",
    "\n",
    "#######################\n",
    "#probability = [False,True]\n",
    "probability = [False] #Too expensive/incositent values (for binary case) => gridsearch verified\n",
    "\n",
    "#class_weight = ['balanced',None]\n",
    "class_weight = ['balanced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgrid = {'cache_size': cache_size, 'kernel':kernel, 'gamma':gamma, \n",
    "         'probability':probability, 'class_weight':class_weight, 'C':C}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=SVC(), param_grid=pgrid, scoring=scoring_labels, refit='f1', n_jobs=-1, cv=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.82,\n",
       " 'cache_size': 1000,\n",
       " 'class_weight': 'balanced',\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'rbf',\n",
       " 'probability': False}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(pos_train,labels_train)\n",
    "\n",
    "svc = grid.best_estimator_ #returns the optimized model (in regards to the parameters)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.82, break_ties=False, cache_size=1000, class_weight='balanced',\n",
       "    coef0=0.0, decision_function_shape='ovr', degree=3, gamma='scale',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(pos_train,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model.predict() and accuracy of our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pred = svc.predict(pos_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 is (mean : 0.652908) and (standard deviation: 0.022871)\n",
      "accuracy is 0.683669\n"
     ]
    }
   ],
   "source": [
    "svc_scores = Compute_scores(svc_pred, labels_test, svc, pos_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_depth = [None] #default\n",
    "#max_depth = [10,20,30,40,50,60,70,80,90,100] #60\n",
    "#max_depth = [55,65] #55\n",
    "#max_depth=[51,52,53,54,55,56,57,58,59,60] #52\n",
    "max_depth=[52]  # ==> OK\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#max_features = ['sqrt','log2'] #sqrt ->31.7\n",
    "#max_features =['sqrt']\n",
    "#max_features = [30,50,100,300] #30\n",
    "#max_features = [20,25,30,35,40,45] #35\n",
    "#max_features=[30,31,32,33,34,35,36,37,38,39] #35\n",
    "max_features = [35] # =====> OK\n",
    "\n",
    "\n",
    "\n",
    "#min_samples_leaf=[1] #default\n",
    "#min_samples_leaf=[1,5,10,20,50,100] #5\n",
    "#min_samples_leaf=[2,3,4,5,6,7,8,9] # 3\n",
    "min_samples_leaf = [3] # ===> OK\n",
    "\n",
    "\n",
    "\n",
    "#min_samples_split=[2] #default\n",
    "#min_samples_split=[2,5,10,30,50,100] #50\n",
    "#min_samples_split = [40,50,60,70,80,90] # 90\n",
    "#min_samples_split =[85,95] #95\n",
    "#min_samples_split=[90,91,92,93,94,95,96,97,98,99] #93\n",
    "min_samples_split=[93] # ==> OK\n",
    "\n",
    "\n",
    "\n",
    "#n_estimators = [100] #default\n",
    "#n_estimators = [10,50,100,150,200,250] #150 or 200(2) or 250 ==> 200\n",
    "#n_estimators = [160,170,180,190,200] # 190\n",
    "#n_estimators = [185,190,195] # 195\n",
    "#n_estimators = [191,192,193,194,195,196,197,198,199] # 199\n",
    "n_estimators = [199] #==> OK\n",
    "\n",
    "#warm_start =[False,True] #False\n",
    "warm_start = [False]\n",
    "\n",
    "\n",
    "#oob_score=[True,False] #False\n",
    "oob_score=[False]\n",
    "\n",
    "n_jobs=[-1]\n",
    "\n",
    "#criterion=['gini','entropy'] #entropy\n",
    "criterion = ['entropy']\n",
    "\n",
    "class_weight = ['balanced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgrid = {'bootstrap':[False], 'max_depth':max_depth, 'max_features':max_features, 'min_samples_leaf':min_samples_leaf, \n",
    "         'min_samples_split': min_samples_split, 'n_estimators':n_estimators,\n",
    "        'class_weight':class_weight, 'criterion':criterion, 'n_jobs':n_jobs,\n",
    "        'oob_score':oob_score, 'warm_start':warm_start}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'class_weight': 'balanced',\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': 52,\n",
       " 'max_features': 35,\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 93,\n",
       " 'n_estimators': 199,\n",
       " 'n_jobs': -1,\n",
       " 'oob_score': False,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=RandomForestClassifier(), param_grid=pgrid, scoring=scoring_labels, refit='f1', cv=10, n_jobs=-1)\n",
    "grid.fit(pos_train,labels_train)\n",
    "\n",
    "rfc = grid.best_estimator_ #returns the optimized model (in regards to the parameters)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight='balanced',\n",
       "                       criterion='entropy', max_depth=52, max_features=35,\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=3, min_samples_split=93,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=199,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(pos_train,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model.predict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pred = rfc.predict(pos_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 is (mean : 0.642633) and (standard deviation: 0.030705)\n",
      "accuracy is 0.680984\n"
     ]
    }
   ],
   "source": [
    "rfc_scores = Compute_scores(rfc_pred, labels_test, rfc, pos_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "Ας εξετάσουμε και αυτόν (με υπολογισμό των hyper parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Υπολογίζουμε <b>hyper-parameters</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alphas = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0] #1.0\n",
    "#alphas = [1.0,10.0,100.0] #1.0\n",
    "#alphas = [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0] #2.0\n",
    "#alphas = [1.5,2.5] #1.5\n",
    "#alphas = [1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9] #1.6\n",
    "#alphas = [1.55,1.65] #1.65\n",
    "#alphas = [1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69] #1.61\n",
    "alphas = [1.61] # ==> OK\n",
    "\n",
    "\n",
    "pgrid = {'alpha': alphas, 'fit_prior': [True],'class_prior': [None] }\n",
    "#Number of priors must match number of classes -> example: [0.1,0.1,0.1,0.1,0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.61, 'class_prior': None, 'fit_prior': True}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=BernoulliNB(), param_grid=pgrid, scoring=scoring_labels, refit='f1', cv=10, n_jobs=-1)\n",
    "grid.fit(pos_train,labels_train)\n",
    "\n",
    "bNB_final = grid.best_estimator_ #returns the optimized model (in regards to the parameters)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 is (mean : 0.654096) and (standard deviation: 0.032611)\n",
      "accuracy is 0.686801\n"
     ]
    }
   ],
   "source": [
    "bNB_final.fit(pos_train,labels_train)\n",
    "bNB_final_pred = bNB_final.predict(pos_test)\n",
    "bNB_final_scores = Compute_scores(bNB_final_pred, labels_test, bNB_final, pos_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Αποτελέσματα"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 (avg)</th>\n",
       "      <th>F1 (τυπική απόκλιση)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NB baseline</th>\n",
       "      <td>0.523490</td>\n",
       "      <td>0.562336</td>\n",
       "      <td>0.027070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB lemmatization</th>\n",
       "      <td>0.522148</td>\n",
       "      <td>0.566353</td>\n",
       "      <td>0.027171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB stopwords</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.592411</td>\n",
       "      <td>0.029309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB bigrams</th>\n",
       "      <td>0.608949</td>\n",
       "      <td>0.571800</td>\n",
       "      <td>0.043224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB Laplace Smoothing</th>\n",
       "      <td>0.534228</td>\n",
       "      <td>0.594627</td>\n",
       "      <td>0.030683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.683669</td>\n",
       "      <td>0.652908</td>\n",
       "      <td>0.022871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.680984</td>\n",
       "      <td>0.642633</td>\n",
       "      <td>0.030705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB final</th>\n",
       "      <td>0.686801</td>\n",
       "      <td>0.654096</td>\n",
       "      <td>0.032611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Accuracy  F1 (avg)  F1 (τυπική απόκλιση)\n",
       "NB baseline           0.523490  0.562336              0.027070\n",
       "NB lemmatization      0.522148  0.566353              0.027171\n",
       "NB stopwords          0.666667  0.592411              0.029309\n",
       "NB bigrams            0.608949  0.571800              0.043224\n",
       "NB Laplace Smoothing  0.534228  0.594627              0.030683\n",
       "SVM                   0.683669  0.652908              0.022871\n",
       "Random Forest         0.680984  0.642633              0.030705\n",
       "NB final              0.686801  0.654096              0.032611"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [bNB1_scores,bNB2_scores,bNB3_scores,bNB4_scores,bNB5_scores,svc_scores,rfc_scores,bNB_final_scores] # a list of lists\n",
    "index1 = ['NB baseline', 'NB lemmatization', 'NB stopwords', 'NB bigrams', 'NB Laplace Smoothing','SVM','Random Forest', 'NB final']\n",
    "columns1 = ['Accuracy','F1 (avg)', 'F1 (τυπική απόκλιση)']\n",
    "\n",
    "NB_df = pd.DataFrame( data=list1, index=index1, columns=columns1)\n",
    "NB_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρήσεις:\n",
    "\n",
    "1) Τα χαμηλά αποτελέσματα μάλλον οφείλονται στο data cleaning ή/και στα 2 features πoυ πρόσθεσα (δεν νομίζω να είναι τα hyperparameters)\n",
    "\n",
    "2) Απο τα 3 τελευταία, το <b> NB final </b> (που είναι τπ πλήρως configured (Bernoulli)Naive Bayes πάνω στον σύνθετο πίνακα μας(tfidf+pos)) έχει τα καλύτερα αποτελέσματα και από θέμα <b> accuracy </b> και από <b> f1_score </b>.\n",
    "\n",
    "3) Βέβαια ως προς <b> accuracy </b>, δεν απέχουν πολύ μεταξύ τους. Ως προς <b> f1 </b> είναι λίγο πιο αισθητή η διαφορά.\n",
    "\n",
    "4) Ενδιαφέρον έχει το standard deviation. Δεν έχει μεγάλη διαφορά για το SVM,Random Forest αλλά είναι μεγαλύτερο του NB_final \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
